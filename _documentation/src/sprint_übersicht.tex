Im folgenden wird der Verlauf der sechs Entwicklungssprints dokumentiert. Die detaillierte technische Dokumentation findet sich in den Einzelkapiteln.

\textbf{Sprint 1 (bis 10.03.2025)} begann mit einer Empfehlung. Ein Lehrbeauftragter schlug vor, MediaPipe als Subprozess für das Kinect-RGB-Signal zu nutzen. Das klang vernünftig. Also wurde ein Dual-Source-System geplant: Kinect und MediaPipe sollten ihre Daten fusionieren, ein Kalman-Filter würde alles glätten. Die Architektur sah auf dem Papier gut durchdacht aus. Beim Choreographie-Meeting am 10. März definierten die Filmakademie-Studenten vier Tracking-Szenarien. Sie wollten Hände tracken, von oben filmen, Bewegungen in Visuals umsetzen. Standard-Anforderungen für ein Motion-Capture-Projekt.

\textbf{Sprint 2 (bis 24.03.2025) }sollte die Datenfusion realisieren. MediaPipe generierte DAT-Tabellen mit x-, y-, z-Koordinaten plus Confidence-Werten. Die Kinect lieferte ihre eigenen Daten. Beide Systeme liefen parallel über dieselbe RGB-Kamera. Der Vergleichstest zeigte: MediaPipe erkannte Körperteile besser, wenn der Tänzer sich drehte. Die Kinect verlor regelmäßig das Skelett. Die Datenfusion funktionierte technisch, aber die Frage drängte sich auf: Warum zwei Systeme nutzen, wenn eines reicht?

\textbf{Sprint 3 (bis 31.03.2025)} war der Visual-Requirements-Sprint. Die Filmakademie brachte Skizzen mit. Abbildung 6 zeigte Kreise um eine Strichfigur – "Skalierungskonzept" stand darunter. Abbildung 7: Kreise außerhalb der Figur – "räumliche Verteilung". Es gab zehn solcher Zeichnungen. Die Aufgabe: Diese Konzepte in TouchDesigner umsetzen. Die Bubbles sollten auf Bewegungen reagieren, magnetisch angezogen werden, sich verteilen. Die technischen Anforderungen wuchsen mit jeder Skizze.

\textbf{Sprint 4 (bis 20.04.2025) }brachte die Wende. Die Kinect wurde entfernt. Drei Wochen Arbeit an der Sensor-Fusion – verworfen. MediaPipe wurde vollständig modularisiert. Die Bubble-Physik erwies sich als komplizierter als erwartet. Erste Implementierung: C\# Shader – zu instabil. Zweite Implementierung: ParticleGPU – zu wenig Kontrolle. Die Bubbles verschwanden zu schnell. Das Problem wurde auf Sprint 5 verschoben.

\textbf{Sprint 5 (bis 07.05.2025)} definierte drei konkrete Visualisierungen. Erstens: Blaue Feuer auf den Händen. ParticleGPU verfolgte die Hand-Nodes. Zweitens: Partikel um den Kopf, die sich veränderten, wenn die Hände über die Schultern gingen. Drittens: Ein Kreis mit Spikes, die auf Hand- und Fußpositionen reagierten. Die 64-Spike-Version ersetzte das ursprüngliche 8-Spike-System. Die Winkelauflösung verbesserte sich von 45° auf 5,625°. Die Screenshots in Abbildung 12 bis 17 zeigen die TouchDesigner-Pipeline. Gelbe Kabel für Koordinatentransformation, magentafarbene für gespiegelte Hand-Verarbeitung.

\textbf{Sprint 6 (bis 12.05.2025)} löste drei produktionskritische Probleme. Problem 1: Die 8 Spikes waren zu grob. Lösung: 64 Spikes. Problem 2: Beamer-Licht überbelichtete die RGB-Kamera. MediaPipe verlor das Tracking. Lösung: Kinect-Infrarot-Stream über OBS Virtual Camera an MediaPipe. Das war eigentlich nicht vorgesehen. MediaPipe ist für RGB optimiert. Es funktionierte trotzdem. Problem 3: Kamera und Beamer standen ungünstig positioniert. Lösung: Parametrische Offset-Korrektur. X/Y-Verschiebung, Rotation, Skalierung – alles in Echtzeit einstellbar.

Am Ende standen sieben Python-Skripte, drei funktionierende Visual-Systeme und eine Infrarot-Pipeline, die niemand geplant hatte. Die Latenz lag unter 20ms. Das 64-Spike-System lief fehlerfrei. Die Hand-Feuer-Effekte funktionierten nach kurzem Setup. Die Kopfpartikel brauchten ständige Kalibrierung, liefen aber.

Der Code liegt auf GitHub. Die Kinect kostet höchstens mit Adaptern zusammen 90 Euro gebraucht. TouchDesigner ist kostenlos für Bildungseinrichtungen. Ein Motion-Capture-System für den Preis eines Lehrbuchs.