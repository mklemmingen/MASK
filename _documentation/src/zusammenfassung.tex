\subsection*{Dokumentationsstruktur}

Diese Dokumentation folgt einem chronologisch-logischen Aufbau, der den Leser vom \textbf{großen Bild zum Detail} führt:

\begin{itemize}
    \item \textbf{Projektüberblick:} Vision, Kooperationskontext und Machbarkeitsstudie
    \item \textbf{Technische Umsetzung:} Anforderungen, Entwicklungsweg und finale Systemarchitektur  
    \item \textbf{Praxisvalidierung:} Produktionsnachweis in der realen Filmproduktion
    \item \textbf{Erkenntnisse:} Zentrale Learnings und Projektbeiträge
    \item \textbf{Technische Dokumentation:} Code-Repository und Teamkommunikation
\end{itemize}

\subsection*{Projektvision}

M.A.S.K. (Machine-Learning Assisted Skeleton Kinect Tracking) entstand aus einer interdisziplinären Kooperation zwischen der Filmakademie Baden-Württemberg und der Hochschule Reutlingen. Das Projekt entwickelte ein kostengünstiges, robustes Motion-Capture-System für die cinematographische Tanzproduktion \textit{Echoes of the Mind}.

Die zentrale Herausforderung bestand in der Entwicklung eines Tracking-Systems, das unter intensiver Beamerbeleuchtung und bei komplexen Bewegungssequenzen zuverlässig funktioniert. Herkömmliche RGB-basierte Tracking-Verfahren zeigen unter diesen Produktionsbedingungen reduzierte Genauigkeit. Zur Lösung wurde eine spezifische Adaptation entwickelt, die Consumer-Hardware und Open-Source-Software kombiniert.

Die Entwicklung durchlief mehrere iterative Phasen: Ein ursprünglich geplanter Dual-Source-Ansatz wurde zugunsten einer vereinfachten MediaPipe-basierten Lösung modifiziert. Die finale Infrarot-Adaptation entstand aus spezifischen Produktionsanforderungen und bewährte sich unter realen Studiobedingungen.

Das resultierende System basiert auf einer modularen Architektur mit TouchDesigner-Containern, die flexible Integration in bestehende Produktions-Workflows ermöglichen. Die Implementation verarbeitet Kamera-Streams in Echtzeit, extrahiert Skelettdaten über MediaPipe und stellt strukturierte Koordinaten für Visualisierungssysteme bereit. Spezialisierte Python-Skripte ermöglichen die direkte Integration in TouchDesigner für ParticleGPU-Effekte, Zustandsmaschinen und Trigger-Logiken.

Das M.A.S.K.-System wurde erfolgreich in einer professionellen Filmproduktion validiert und demonstriert die Wirksamkeit kostengünstiger Motion-Capture-Lösungen für spezialisierte Anwendungen. Die modulare Systemarchitektur ermöglicht Reproduzierbarkeit und Erweiterung für ähnliche interdisziplinäre Projekte.

Hervorgebracht wurden zusätzlich Schwierigkeiten bei der Verarbeitung von teilweise verdeckten Performern. Wenn die grundlegenden RGB-Daten nicht ausreichend schlüssig für das MediaPipe-Modell waren, folgte das System einen GIGO-Prinzip: Garbage In, Garbage Out. Das System konnte dann keine sinnvollen Skelettdaten mehr extrahieren. Die finale Infrarot-Adaptation ermöglichte es, in Momenten wo der Beamer auf dem Tänzer ein Problem war, auch bei partieller Sichtbarkeit präzisere Tracking-Daten zu generieren.

Die Entwicklung verdeutlicht das Potenzial interdisziplinärer Kooperationen zwischen technischen Hochschulen und Kunsthochschulen für praxisorientierte Lösungsansätze in der Creative Technology.

\textbf{Open Source:} Der vollständige Quellcode, inklusive TouchDesigner-Projekten und Python-Skripten, sowie dem LaTex dieser Dokumentation, ist verfügbar unter:

\texttt{https://github.com/mklemmingen/MASK}

\textbf{Behind-The-Scenes Video:} Visualisierungen, Debug-Overlays und Setup der Infrarot-Pipeline unter: \texttt{TODO LINK EINFÜGEN}